# ğŸš€ Using Local Files (MUCH FASTER!)

## Why Use Local Files?

If your friend downloads the files from Google Drive/Dropbox to their computer, the script can read from those local files instead of downloading from S3. This is **10-100x FASTER** because:

- âœ… **No network latency** (Netherlands â†’ US = 100-200ms per request)
- âœ… **No internet needed** after files are downloaded
- âœ… **Much faster processing** (30 seconds vs 15-25 minutes per year!)

## How to Set It Up

### Step 1: Download Files from Drive/Dropbox

Your friend needs to download the files and organize them like this:

```
downloaded_data/
â”œâ”€â”€ 2016/
â”‚   â”œâ”€â”€ 01/
â”‚   â”‚   â”œâ”€â”€ 2016-01-04.csv.gz
â”‚   â”‚   â”œâ”€â”€ 2016-01-05.csv.gz
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ 02/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ ...
â”œâ”€â”€ 2017/
â”‚   â””â”€â”€ ...
â””â”€â”€ 2018/
    â””â”€â”€ ...
```

**OR** any structure where files are organized by year/month.

### Step 2: Update the Script

Open `aggregate.py` and find these lines (around line 131-133):

```python
USE_LOCAL_FILES = False  # â† Set to True if files are downloaded locally
LOCAL_FILES_PATH = "./downloaded_data"  # â† Path to folder with downloaded files
```

**Change to:**

```python
USE_LOCAL_FILES = True  # â† Changed to True!
LOCAL_FILES_PATH = "./downloaded_data"  # â† Change this to where your files are
```

**If files are in a different location, update the path:**
- `LOCAL_FILES_PATH = "./data"`  (if in a "data" folder)
- `LOCAL_FILES_PATH = "/Users/YourName/Downloads/options_data"`  (full path)
- `LOCAL_FILES_PATH = "C:\\Users\\YourName\\Downloads\\options_data"`  (Windows path)

### Step 3: Run the Script

That's it! Just run the script normally:

```bash
cd python-boilerplate
python3 -u src/backtesting/data/aggregate.py
```

The script will automatically:
- âœ… Detect that you're using local files
- âœ… Skip S3 setup (no internet needed!)
- âœ… Read from your local files (super fast!)
- âœ… Process everything in 30 seconds to 2 minutes (instead of 15-25 minutes!)

## Expected Speed

| Method | Time per Year | Notes |
|--------|---------------|-------|
| **Local Files** | **30 sec - 2 min** | âš¡ Super fast! |
| S3 (US user) | 3-5 minutes | Good, but slower |
| S3 (Netherlands) | 15-25 minutes | Slow due to latency |

## File Structure Options

The script will automatically try to find files in these patterns:

1. `LOCAL_FILES_PATH/2017/*/*.csv.gz` (year/month/files)
2. `LOCAL_FILES_PATH/*/*/*.csv.gz` (any structure)
3. `LOCAL_FILES_PATH/*2017*/*/*.csv.gz` (year in folder name)

So your friend doesn't need perfect organization - the script will find the files!

## Troubleshooting

### "No local files found"
- Check that `LOCAL_FILES_PATH` points to the right folder
- Make sure files are `.csv.gz` format
- Try using the full path instead of relative path

### "Path does not exist"
- Make sure the folder exists
- Check spelling of the path
- On Windows, use `\\` instead of `/` or use forward slashes `/`

### Still slow?
- Make sure `USE_LOCAL_FILES = True` (not `False`)
- Check that files are actually on local disk (not network drive)
- SSD is faster than HDD, but both work

## Summary

**For your friend:**
1. Download files from Drive/Dropbox
2. Set `USE_LOCAL_FILES = True`
3. Set `LOCAL_FILES_PATH` to the folder location
4. Run script - it's now 10-100x faster! ğŸš€











